{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于多因子数据股价预测的PatchTST \n",
    "\n",
    "## 1、引入依赖库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # **************** UNCOMMENT AND RUN THIS CELL IF YOU NEED TO INSTALL/ UPGRADE TSAI ****************\n",
    "stable = True # Set to True for latest pip version or False for main branch in GitHub\n",
    "!pip install {\"tsai -U\" if stable else \"git+https://github.com/timeseriesAI/tsai.git\"} >> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from tsai.basics import *\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、数据准备\n",
    "\n",
    "在训练之前，需要将数据转换为模型要求的指定的格式：\n",
    "\n",
    "1. 将数据读取成pandas的DataFrame结构，其中包含要预测的变量\n",
    "2. 对数据进行预处理（缺失值、极值、标准化）\n",
    "3. 定义训练集、测试集和验证集的划分\n",
    "4. 使用train split函数对数据进行拆分\n",
    "5. 应用滑动窗口来准备输入和输出的数据\n",
    "\n",
    "### 2.1 准备DataFrame数据\n",
    "\n",
    "在本研究中，我们使用沪深300指数中股票的多因子历史数据，整理来说，此任务是一个多变量长期时序预测问题（Long-term time series forecasting, LTSF）, 利用多个历史时间步长的多变量数据预测多个未来时间步长的单变量数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/selected_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>asset</th>\n",
       "      <th>asset_impairment_loss_ttm</th>\n",
       "      <th>cash_flow_to_price_ratio</th>\n",
       "      <th>operating_revenue_per_share</th>\n",
       "      <th>cumulative_range</th>\n",
       "      <th>daily_standard_deviation</th>\n",
       "      <th>debt_to_assets</th>\n",
       "      <th>TRIX5</th>\n",
       "      <th>TRIX10</th>\n",
       "      <th>...</th>\n",
       "      <th>Variance60</th>\n",
       "      <th>equity_to_fixed_asset_ratio</th>\n",
       "      <th>average_share_turnover_annual</th>\n",
       "      <th>debt_to_tangible_equity_ratio</th>\n",
       "      <th>debt_to_equity_ratio</th>\n",
       "      <th>debt_to_asset_ratio</th>\n",
       "      <th>average_share_turnover_quarterly</th>\n",
       "      <th>beta</th>\n",
       "      <th>account_receivable_turnover_rate</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>000001.XSHE</td>\n",
       "      <td>1.082840</td>\n",
       "      <td>-1.782851</td>\n",
       "      <td>0.138523</td>\n",
       "      <td>-0.015082</td>\n",
       "      <td>-0.130014</td>\n",
       "      <td>0.949743</td>\n",
       "      <td>-0.669114</td>\n",
       "      <td>-0.467289</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059143</td>\n",
       "      <td>0.389722</td>\n",
       "      <td>-0.627465</td>\n",
       "      <td>0.873913</td>\n",
       "      <td>0.907126</td>\n",
       "      <td>0.948828</td>\n",
       "      <td>-0.743923</td>\n",
       "      <td>-0.474340</td>\n",
       "      <td>-1.569855e-13</td>\n",
       "      <td>-0.175372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>000002.XSHE</td>\n",
       "      <td>1.872958</td>\n",
       "      <td>0.550676</td>\n",
       "      <td>0.009427</td>\n",
       "      <td>-0.512981</td>\n",
       "      <td>-0.524570</td>\n",
       "      <td>0.313397</td>\n",
       "      <td>0.012472</td>\n",
       "      <td>0.198167</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.542034</td>\n",
       "      <td>0.462059</td>\n",
       "      <td>-0.318455</td>\n",
       "      <td>0.058255</td>\n",
       "      <td>0.305223</td>\n",
       "      <td>0.311860</td>\n",
       "      <td>-0.109738</td>\n",
       "      <td>-0.282214</td>\n",
       "      <td>-2.677858e-13</td>\n",
       "      <td>-0.746300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>000009.XSHE</td>\n",
       "      <td>-0.408687</td>\n",
       "      <td>0.564045</td>\n",
       "      <td>-0.124014</td>\n",
       "      <td>-0.486472</td>\n",
       "      <td>-0.092968</td>\n",
       "      <td>0.057058</td>\n",
       "      <td>-0.044310</td>\n",
       "      <td>-0.172772</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.166335</td>\n",
       "      <td>-1.426181</td>\n",
       "      <td>0.304952</td>\n",
       "      <td>-0.175558</td>\n",
       "      <td>0.215119</td>\n",
       "      <td>0.030641</td>\n",
       "      <td>0.178116</td>\n",
       "      <td>-0.207993</td>\n",
       "      <td>-3.752554e-14</td>\n",
       "      <td>-0.764651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>000012.XSHE</td>\n",
       "      <td>1.574693</td>\n",
       "      <td>-0.597120</td>\n",
       "      <td>-1.334493</td>\n",
       "      <td>-0.020186</td>\n",
       "      <td>0.684936</td>\n",
       "      <td>-0.775392</td>\n",
       "      <td>0.037010</td>\n",
       "      <td>-0.086281</td>\n",
       "      <td>...</td>\n",
       "      <td>0.312496</td>\n",
       "      <td>-0.702248</td>\n",
       "      <td>0.382009</td>\n",
       "      <td>-1.308295</td>\n",
       "      <td>-1.195322</td>\n",
       "      <td>-0.799087</td>\n",
       "      <td>0.290596</td>\n",
       "      <td>1.094184</td>\n",
       "      <td>-5.084821e-14</td>\n",
       "      <td>0.021916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>000021.XSHE</td>\n",
       "      <td>-0.212554</td>\n",
       "      <td>0.028205</td>\n",
       "      <td>0.873796</td>\n",
       "      <td>0.739504</td>\n",
       "      <td>0.271701</td>\n",
       "      <td>-0.815003</td>\n",
       "      <td>-0.463916</td>\n",
       "      <td>-1.281419</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024420</td>\n",
       "      <td>-0.353840</td>\n",
       "      <td>-0.065789</td>\n",
       "      <td>-0.425548</td>\n",
       "      <td>-0.451862</td>\n",
       "      <td>-0.829157</td>\n",
       "      <td>-0.581038</td>\n",
       "      <td>0.157243</td>\n",
       "      <td>-2.708944e-14</td>\n",
       "      <td>-0.036210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date        asset  asset_impairment_loss_ttm  \\\n",
       "0  2010-01-04  000001.XSHE                   1.082840   \n",
       "1  2010-01-04  000002.XSHE                   1.872958   \n",
       "2  2010-01-04  000009.XSHE                  -0.408687   \n",
       "3  2010-01-04  000012.XSHE                   1.574693   \n",
       "4  2010-01-04  000021.XSHE                  -0.212554   \n",
       "\n",
       "   cash_flow_to_price_ratio  operating_revenue_per_share  cumulative_range  \\\n",
       "0                 -1.782851                     0.138523         -0.015082   \n",
       "1                  0.550676                     0.009427         -0.512981   \n",
       "2                  0.564045                    -0.124014         -0.486472   \n",
       "3                 -0.597120                    -1.334493         -0.020186   \n",
       "4                  0.028205                     0.873796          0.739504   \n",
       "\n",
       "   daily_standard_deviation  debt_to_assets     TRIX5    TRIX10  ...  \\\n",
       "0                 -0.130014        0.949743 -0.669114 -0.467289  ...   \n",
       "1                 -0.524570        0.313397  0.012472  0.198167  ...   \n",
       "2                 -0.092968        0.057058 -0.044310 -0.172772  ...   \n",
       "3                  0.684936       -0.775392  0.037010 -0.086281  ...   \n",
       "4                  0.271701       -0.815003 -0.463916 -1.281419  ...   \n",
       "\n",
       "   Variance60  equity_to_fixed_asset_ratio  average_share_turnover_annual  \\\n",
       "0   -0.059143                     0.389722                      -0.627465   \n",
       "1   -0.542034                     0.462059                      -0.318455   \n",
       "2   -0.166335                    -1.426181                       0.304952   \n",
       "3    0.312496                    -0.702248                       0.382009   \n",
       "4   -0.024420                    -0.353840                      -0.065789   \n",
       "\n",
       "   debt_to_tangible_equity_ratio  debt_to_equity_ratio  debt_to_asset_ratio  \\\n",
       "0                       0.873913              0.907126             0.948828   \n",
       "1                       0.058255              0.305223             0.311860   \n",
       "2                      -0.175558              0.215119             0.030641   \n",
       "3                      -1.308295             -1.195322            -0.799087   \n",
       "4                      -0.425548             -0.451862            -0.829157   \n",
       "\n",
       "   average_share_turnover_quarterly      beta  \\\n",
       "0                         -0.743923 -0.474340   \n",
       "1                         -0.109738 -0.282214   \n",
       "2                          0.178116 -0.207993   \n",
       "3                          0.290596  1.094184   \n",
       "4                         -0.581038  0.157243   \n",
       "\n",
       "   account_receivable_turnover_rate     price  \n",
       "0                     -1.569855e-13 -0.175372  \n",
       "1                     -2.677858e-13 -0.746300  \n",
       "2                     -3.752554e-14 -0.764651  \n",
       "3                     -5.084821e-14  0.021916  \n",
       "4                     -2.708944e-14 -0.036210  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 数据预处理\n",
    "\n",
    "对数据进行预处理，在输入模型前对因子数据再一次进行缺失值、重复值处理。tsai库提供了一些sklearn风格的转换函数，用于构建预处理管道。在本研究中，我们使用以下转换函数：\n",
    "\n",
    "1. TSShrinkDataFrame: to save some memory and set the right dtypes.\n",
    "2. TSDropDuplicates: to ensure there are no duplicate timestamps.\n",
    "3. TSAddMissingTimestamps: to fill any missing timestamps.\n",
    "4. TSFillMissing: to fill any missing data (forward fill, then 0)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小Tips：按照股票代码对原始数据分组，取出每个股票代码的历史因子数据，分别对每个数据进行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data directory already exists.\n",
      "Pipeline saved as data/preproc_pipe.pkl\n",
      "Initial memory usage: 365.67 MB \n",
      "Final memory usage  : 179.48 MB  (-50.9%)\n",
      "[Pipeline] .......... (step 1 of 2) Processing shrinker, total=   0.6s\n",
      "[Pipeline] ...... (step 2 of 2) Processing fill_missing, total=   2.8s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>asset</th>\n",
       "      <th>asset_impairment_loss_ttm</th>\n",
       "      <th>cash_flow_to_price_ratio</th>\n",
       "      <th>operating_revenue_per_share</th>\n",
       "      <th>cumulative_range</th>\n",
       "      <th>daily_standard_deviation</th>\n",
       "      <th>debt_to_assets</th>\n",
       "      <th>TRIX5</th>\n",
       "      <th>TRIX10</th>\n",
       "      <th>...</th>\n",
       "      <th>Variance60</th>\n",
       "      <th>equity_to_fixed_asset_ratio</th>\n",
       "      <th>average_share_turnover_annual</th>\n",
       "      <th>debt_to_tangible_equity_ratio</th>\n",
       "      <th>debt_to_equity_ratio</th>\n",
       "      <th>debt_to_asset_ratio</th>\n",
       "      <th>average_share_turnover_quarterly</th>\n",
       "      <th>beta</th>\n",
       "      <th>account_receivable_turnover_rate</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>000001.XSHE</td>\n",
       "      <td>1.082840</td>\n",
       "      <td>-1.782851</td>\n",
       "      <td>0.138523</td>\n",
       "      <td>-0.015082</td>\n",
       "      <td>-0.130014</td>\n",
       "      <td>0.949743</td>\n",
       "      <td>-0.669114</td>\n",
       "      <td>-0.467289</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059143</td>\n",
       "      <td>0.389722</td>\n",
       "      <td>-0.627465</td>\n",
       "      <td>0.873913</td>\n",
       "      <td>0.907126</td>\n",
       "      <td>0.948828</td>\n",
       "      <td>-0.743923</td>\n",
       "      <td>-0.474340</td>\n",
       "      <td>-1.569855e-13</td>\n",
       "      <td>-0.175372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>000002.XSHE</td>\n",
       "      <td>1.872958</td>\n",
       "      <td>0.550676</td>\n",
       "      <td>0.009427</td>\n",
       "      <td>-0.512981</td>\n",
       "      <td>-0.524570</td>\n",
       "      <td>0.313397</td>\n",
       "      <td>0.012472</td>\n",
       "      <td>0.198167</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.542034</td>\n",
       "      <td>0.462059</td>\n",
       "      <td>-0.318455</td>\n",
       "      <td>0.058255</td>\n",
       "      <td>0.305223</td>\n",
       "      <td>0.311860</td>\n",
       "      <td>-0.109738</td>\n",
       "      <td>-0.282214</td>\n",
       "      <td>-2.677858e-13</td>\n",
       "      <td>-0.746300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>000009.XSHE</td>\n",
       "      <td>-0.408687</td>\n",
       "      <td>0.564045</td>\n",
       "      <td>-0.124014</td>\n",
       "      <td>-0.486472</td>\n",
       "      <td>-0.092968</td>\n",
       "      <td>0.057058</td>\n",
       "      <td>-0.044310</td>\n",
       "      <td>-0.172772</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.166335</td>\n",
       "      <td>-1.426181</td>\n",
       "      <td>0.304952</td>\n",
       "      <td>-0.175558</td>\n",
       "      <td>0.215119</td>\n",
       "      <td>0.030641</td>\n",
       "      <td>0.178116</td>\n",
       "      <td>-0.207993</td>\n",
       "      <td>-3.752554e-14</td>\n",
       "      <td>-0.764651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>000012.XSHE</td>\n",
       "      <td>1.574693</td>\n",
       "      <td>-0.597120</td>\n",
       "      <td>-1.334493</td>\n",
       "      <td>-0.020186</td>\n",
       "      <td>0.684936</td>\n",
       "      <td>-0.775392</td>\n",
       "      <td>0.037010</td>\n",
       "      <td>-0.086281</td>\n",
       "      <td>...</td>\n",
       "      <td>0.312496</td>\n",
       "      <td>-0.702248</td>\n",
       "      <td>0.382009</td>\n",
       "      <td>-1.308295</td>\n",
       "      <td>-1.195322</td>\n",
       "      <td>-0.799087</td>\n",
       "      <td>0.290596</td>\n",
       "      <td>1.094184</td>\n",
       "      <td>-5.084821e-14</td>\n",
       "      <td>0.021916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>000021.XSHE</td>\n",
       "      <td>-0.212554</td>\n",
       "      <td>0.028205</td>\n",
       "      <td>0.873796</td>\n",
       "      <td>0.739504</td>\n",
       "      <td>0.271701</td>\n",
       "      <td>-0.815003</td>\n",
       "      <td>-0.463915</td>\n",
       "      <td>-1.281419</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024420</td>\n",
       "      <td>-0.353840</td>\n",
       "      <td>-0.065789</td>\n",
       "      <td>-0.425548</td>\n",
       "      <td>-0.451862</td>\n",
       "      <td>-0.829157</td>\n",
       "      <td>-0.581038</td>\n",
       "      <td>0.157243</td>\n",
       "      <td>-2.708944e-14</td>\n",
       "      <td>-0.036210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date        asset  asset_impairment_loss_ttm  \\\n",
       "0  2010-01-04  000001.XSHE                   1.082840   \n",
       "1  2010-01-04  000002.XSHE                   1.872958   \n",
       "2  2010-01-04  000009.XSHE                  -0.408687   \n",
       "3  2010-01-04  000012.XSHE                   1.574693   \n",
       "4  2010-01-04  000021.XSHE                  -0.212554   \n",
       "\n",
       "   cash_flow_to_price_ratio  operating_revenue_per_share  cumulative_range  \\\n",
       "0                 -1.782851                     0.138523         -0.015082   \n",
       "1                  0.550676                     0.009427         -0.512981   \n",
       "2                  0.564045                    -0.124014         -0.486472   \n",
       "3                 -0.597120                    -1.334493         -0.020186   \n",
       "4                  0.028205                     0.873796          0.739504   \n",
       "\n",
       "   daily_standard_deviation  debt_to_assets     TRIX5    TRIX10  ...  \\\n",
       "0                 -0.130014        0.949743 -0.669114 -0.467289  ...   \n",
       "1                 -0.524570        0.313397  0.012472  0.198167  ...   \n",
       "2                 -0.092968        0.057058 -0.044310 -0.172772  ...   \n",
       "3                  0.684936       -0.775392  0.037010 -0.086281  ...   \n",
       "4                  0.271701       -0.815003 -0.463915 -1.281419  ...   \n",
       "\n",
       "   Variance60  equity_to_fixed_asset_ratio  average_share_turnover_annual  \\\n",
       "0   -0.059143                     0.389722                      -0.627465   \n",
       "1   -0.542034                     0.462059                      -0.318455   \n",
       "2   -0.166335                    -1.426181                       0.304952   \n",
       "3    0.312496                    -0.702248                       0.382009   \n",
       "4   -0.024420                    -0.353840                      -0.065789   \n",
       "\n",
       "   debt_to_tangible_equity_ratio  debt_to_equity_ratio  debt_to_asset_ratio  \\\n",
       "0                       0.873913              0.907126             0.948828   \n",
       "1                       0.058255              0.305223             0.311860   \n",
       "2                      -0.175558              0.215119             0.030641   \n",
       "3                      -1.308295             -1.195322            -0.799087   \n",
       "4                      -0.425548             -0.451862            -0.829157   \n",
       "\n",
       "   average_share_turnover_quarterly      beta  \\\n",
       "0                         -0.743923 -0.474340   \n",
       "1                         -0.109738 -0.282214   \n",
       "2                          0.178116 -0.207993   \n",
       "3                          0.290596  1.094184   \n",
       "4                         -0.581038  0.157243   \n",
       "\n",
       "   account_receivable_turnover_rate     price  \n",
       "0                     -1.569855e-13 -0.175372  \n",
       "1                     -2.677858e-13 -0.746300  \n",
       "2                     -3.752554e-14 -0.764651  \n",
       "3                     -5.084821e-14  0.021916  \n",
       "4                     -2.708944e-14 -0.036210  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime_col = \"date\"\n",
    "freq = '1D'\n",
    "columns = dataset.columns[2:]\n",
    "method = 'ffill'\n",
    "value = 0\n",
    "\n",
    "# pipeline\n",
    "preproc_pipe = sklearn.pipeline.Pipeline([\n",
    "    ('shrinker', TSShrinkDataFrame()), # shrink dataframe memory usage\n",
    "    ('fill_missing', TSFillMissing(columns=columns, method=method, value=value)), # fill missing data (1st ffill. 2nd value=0)\n",
    "    ], \n",
    "    verbose=True)\n",
    "mkdir('data', exist_ok=True, parents=True)\n",
    "save_object(preproc_pipe, 'data/preproc_pipe.pkl')\n",
    "preproc_pipe = load_object('data/preproc_pipe.pkl')\n",
    "\n",
    "dataset = preproc_pipe.fit_transform(dataset)\n",
    "# dataset = dataset.sort_values(by=['date', 'asset'])\n",
    "dataset.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 定义数据拆分\n",
    "\n",
    "将具有 个时间步长和  个特征（不包括日期时间）的多元时间序列转换为：\n",
    "\n",
    "- 个输入样本，包含 个因子特征值和 历史时间步长\n",
    "- 个输入样本，包含 个因子特征值和 未来时间步长\n",
    "\n",
    "我们使用tsai库中的get_forecasting_splits函数实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset.reset_index(inplace=True)\n",
    "dataset.set_index(['date', 'asset'], inplace=True)\n",
    "dataset.drop(columns=['index'], inplace=True)\n",
    "factor_names = dataset.columns.tolist()\n",
    "stock_codes = dataset.index.levels[1].tolist()\n",
    "trade_dates = dataset.index.levels[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 372, 2431)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(factor_names), len(stock_codes), len(trade_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>asset_impairment_loss_ttm</th>\n",
       "      <th>cash_flow_to_price_ratio</th>\n",
       "      <th>operating_revenue_per_share</th>\n",
       "      <th>cumulative_range</th>\n",
       "      <th>daily_standard_deviation</th>\n",
       "      <th>debt_to_assets</th>\n",
       "      <th>TRIX5</th>\n",
       "      <th>TRIX10</th>\n",
       "      <th>single_day_VPT_6</th>\n",
       "      <th>single_day_VPT_12</th>\n",
       "      <th>...</th>\n",
       "      <th>Variance60</th>\n",
       "      <th>equity_to_fixed_asset_ratio</th>\n",
       "      <th>average_share_turnover_annual</th>\n",
       "      <th>debt_to_tangible_equity_ratio</th>\n",
       "      <th>debt_to_equity_ratio</th>\n",
       "      <th>debt_to_asset_ratio</th>\n",
       "      <th>average_share_turnover_quarterly</th>\n",
       "      <th>beta</th>\n",
       "      <th>account_receivable_turnover_rate</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th>asset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2010-01-04</th>\n",
       "      <th>000001.XSHE</th>\n",
       "      <td>1.082840</td>\n",
       "      <td>-1.782851</td>\n",
       "      <td>0.138523</td>\n",
       "      <td>-0.015082</td>\n",
       "      <td>-0.130014</td>\n",
       "      <td>0.949743</td>\n",
       "      <td>-0.669114</td>\n",
       "      <td>-0.467289</td>\n",
       "      <td>0.629844</td>\n",
       "      <td>-1.044537</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059143</td>\n",
       "      <td>0.389722</td>\n",
       "      <td>-0.627465</td>\n",
       "      <td>0.873913</td>\n",
       "      <td>0.907126</td>\n",
       "      <td>0.948828</td>\n",
       "      <td>-0.743923</td>\n",
       "      <td>-0.474340</td>\n",
       "      <td>-1.569855e-13</td>\n",
       "      <td>-0.175372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000002.XSHE</th>\n",
       "      <td>1.872958</td>\n",
       "      <td>0.550676</td>\n",
       "      <td>0.009427</td>\n",
       "      <td>-0.512981</td>\n",
       "      <td>-0.524570</td>\n",
       "      <td>0.313397</td>\n",
       "      <td>0.012472</td>\n",
       "      <td>0.198167</td>\n",
       "      <td>1.460589</td>\n",
       "      <td>-1.034271</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.542034</td>\n",
       "      <td>0.462059</td>\n",
       "      <td>-0.318455</td>\n",
       "      <td>0.058255</td>\n",
       "      <td>0.305223</td>\n",
       "      <td>0.311860</td>\n",
       "      <td>-0.109738</td>\n",
       "      <td>-0.282214</td>\n",
       "      <td>-2.677858e-13</td>\n",
       "      <td>-0.746300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000009.XSHE</th>\n",
       "      <td>-0.408687</td>\n",
       "      <td>0.564045</td>\n",
       "      <td>-0.124014</td>\n",
       "      <td>-0.486472</td>\n",
       "      <td>-0.092968</td>\n",
       "      <td>0.057058</td>\n",
       "      <td>-0.044310</td>\n",
       "      <td>-0.172772</td>\n",
       "      <td>0.194103</td>\n",
       "      <td>0.466682</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.166335</td>\n",
       "      <td>-1.426181</td>\n",
       "      <td>0.304952</td>\n",
       "      <td>-0.175558</td>\n",
       "      <td>0.215119</td>\n",
       "      <td>0.030641</td>\n",
       "      <td>0.178116</td>\n",
       "      <td>-0.207993</td>\n",
       "      <td>-3.752554e-14</td>\n",
       "      <td>-0.764651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000012.XSHE</th>\n",
       "      <td>1.574693</td>\n",
       "      <td>-0.597120</td>\n",
       "      <td>-1.334493</td>\n",
       "      <td>-0.020186</td>\n",
       "      <td>0.684936</td>\n",
       "      <td>-0.775392</td>\n",
       "      <td>0.037010</td>\n",
       "      <td>-0.086281</td>\n",
       "      <td>0.160596</td>\n",
       "      <td>-0.628887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.312496</td>\n",
       "      <td>-0.702248</td>\n",
       "      <td>0.382009</td>\n",
       "      <td>-1.308295</td>\n",
       "      <td>-1.195322</td>\n",
       "      <td>-0.799087</td>\n",
       "      <td>0.290596</td>\n",
       "      <td>1.094184</td>\n",
       "      <td>-5.084821e-14</td>\n",
       "      <td>0.021916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000021.XSHE</th>\n",
       "      <td>-0.212554</td>\n",
       "      <td>0.028205</td>\n",
       "      <td>0.873796</td>\n",
       "      <td>0.739504</td>\n",
       "      <td>0.271701</td>\n",
       "      <td>-0.815003</td>\n",
       "      <td>-0.463915</td>\n",
       "      <td>-1.281419</td>\n",
       "      <td>0.179706</td>\n",
       "      <td>-0.441030</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024420</td>\n",
       "      <td>-0.353840</td>\n",
       "      <td>-0.065789</td>\n",
       "      <td>-0.425548</td>\n",
       "      <td>-0.451862</td>\n",
       "      <td>-0.829157</td>\n",
       "      <td>-0.581038</td>\n",
       "      <td>0.157243</td>\n",
       "      <td>-2.708944e-14</td>\n",
       "      <td>-0.036210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        asset_impairment_loss_ttm  cash_flow_to_price_ratio  \\\n",
       "date       asset                                                              \n",
       "2010-01-04 000001.XSHE                   1.082840                 -1.782851   \n",
       "           000002.XSHE                   1.872958                  0.550676   \n",
       "           000009.XSHE                  -0.408687                  0.564045   \n",
       "           000012.XSHE                   1.574693                 -0.597120   \n",
       "           000021.XSHE                  -0.212554                  0.028205   \n",
       "\n",
       "                        operating_revenue_per_share  cumulative_range  \\\n",
       "date       asset                                                        \n",
       "2010-01-04 000001.XSHE                     0.138523         -0.015082   \n",
       "           000002.XSHE                     0.009427         -0.512981   \n",
       "           000009.XSHE                    -0.124014         -0.486472   \n",
       "           000012.XSHE                    -1.334493         -0.020186   \n",
       "           000021.XSHE                     0.873796          0.739504   \n",
       "\n",
       "                        daily_standard_deviation  debt_to_assets     TRIX5  \\\n",
       "date       asset                                                             \n",
       "2010-01-04 000001.XSHE                 -0.130014        0.949743 -0.669114   \n",
       "           000002.XSHE                 -0.524570        0.313397  0.012472   \n",
       "           000009.XSHE                 -0.092968        0.057058 -0.044310   \n",
       "           000012.XSHE                  0.684936       -0.775392  0.037010   \n",
       "           000021.XSHE                  0.271701       -0.815003 -0.463915   \n",
       "\n",
       "                          TRIX10  single_day_VPT_6  single_day_VPT_12  ...  \\\n",
       "date       asset                                                       ...   \n",
       "2010-01-04 000001.XSHE -0.467289          0.629844          -1.044537  ...   \n",
       "           000002.XSHE  0.198167          1.460589          -1.034271  ...   \n",
       "           000009.XSHE -0.172772          0.194103           0.466682  ...   \n",
       "           000012.XSHE -0.086281          0.160596          -0.628887  ...   \n",
       "           000021.XSHE -1.281419          0.179706          -0.441030  ...   \n",
       "\n",
       "                        Variance60  equity_to_fixed_asset_ratio  \\\n",
       "date       asset                                                  \n",
       "2010-01-04 000001.XSHE   -0.059143                     0.389722   \n",
       "           000002.XSHE   -0.542034                     0.462059   \n",
       "           000009.XSHE   -0.166335                    -1.426181   \n",
       "           000012.XSHE    0.312496                    -0.702248   \n",
       "           000021.XSHE   -0.024420                    -0.353840   \n",
       "\n",
       "                        average_share_turnover_annual  \\\n",
       "date       asset                                        \n",
       "2010-01-04 000001.XSHE                      -0.627465   \n",
       "           000002.XSHE                      -0.318455   \n",
       "           000009.XSHE                       0.304952   \n",
       "           000012.XSHE                       0.382009   \n",
       "           000021.XSHE                      -0.065789   \n",
       "\n",
       "                        debt_to_tangible_equity_ratio  debt_to_equity_ratio  \\\n",
       "date       asset                                                              \n",
       "2010-01-04 000001.XSHE                       0.873913              0.907126   \n",
       "           000002.XSHE                       0.058255              0.305223   \n",
       "           000009.XSHE                      -0.175558              0.215119   \n",
       "           000012.XSHE                      -1.308295             -1.195322   \n",
       "           000021.XSHE                      -0.425548             -0.451862   \n",
       "\n",
       "                        debt_to_asset_ratio  average_share_turnover_quarterly  \\\n",
       "date       asset                                                                \n",
       "2010-01-04 000001.XSHE             0.948828                         -0.743923   \n",
       "           000002.XSHE             0.311860                         -0.109738   \n",
       "           000009.XSHE             0.030641                          0.178116   \n",
       "           000012.XSHE            -0.799087                          0.290596   \n",
       "           000021.XSHE            -0.829157                         -0.581038   \n",
       "\n",
       "                            beta  account_receivable_turnover_rate     price  \n",
       "date       asset                                                              \n",
       "2010-01-04 000001.XSHE -0.474340                     -1.569855e-13 -0.175372  \n",
       "           000002.XSHE -0.282214                     -2.677858e-13 -0.746300  \n",
       "           000009.XSHE -0.207993                     -3.752554e-14 -0.764651  \n",
       "           000012.XSHE  1.094184                     -5.084821e-14  0.021916  \n",
       "           000021.XSHE  0.157243                     -2.708944e-14 -0.036210  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 根据滑动窗口的大小准备模型输入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x50 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fcst_history = 120 # steps for the past\n",
    "fcst_horizon = 30  # steps for the future\n",
    "\n",
    "valid_size   = 0.1  # int or float indicating the size of the training set\n",
    "test_size    = 0.2  # int or float indicating the size of the test set\n",
    "\n",
    "stock = stock_codes[0]\n",
    "df_show = dataset.loc[(slice(None), stock), :]\n",
    "df_show.reset_index(inplace=True)\n",
    "\n",
    "split = get_forecasting_splits(df_show, fcst_history=fcst_history, fcst_horizon=fcst_horizon, datetime_col=datetime_col,\n",
    "                                valid_size=valid_size, test_size=test_size)\n",
    "train_split, valid_split, test_split = split\n",
    "add_len= len(train_split)+len(valid_split)+len(test_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data directory already exists.\n",
      "Pipeline saved as data/exp_pipe.pkl\n"
     ]
    }
   ],
   "source": [
    "columns = df_show.columns[2:]\n",
    "# pipeline\n",
    "exp_pipe = sklearn.pipeline.Pipeline([\n",
    "    ('scaler', TSStandardScaler(columns=columns)), # standardize data using train_split\n",
    "    ], \n",
    "    verbose=True)\n",
    "save_object(exp_pipe, 'data/exp_pipe.pkl')\n",
    "exp_pipe = load_object('data/exp_pipe.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th stock:  000001.XSHE\n",
      "[Pipeline] ............ (step 1 of 1) Processing scaler, total=   0.0s\n",
      "1th stock:  000002.XSHE\n",
      "[Pipeline] ............ (step 1 of 1) Processing scaler, total=   0.0s\n",
      "2th stock:  000009.XSHE\n",
      "[Pipeline] ............ (step 1 of 1) Processing scaler, total=   0.0s\n",
      "3th stock:  000012.XSHE\n",
      "[Pipeline] ............ (step 1 of 1) Processing scaler, total=   0.0s\n",
      "4th stock:  000021.XSHE\n",
      "[Pipeline] ............ (step 1 of 1) Processing scaler, total=   0.0s\n",
      "5th stock:  000027.XSHE\n",
      "[Pipeline] ............ (step 1 of 1) Processing scaler, total=   0.0s\n",
      "6th stock:  000031.XSHE\n",
      "[Pipeline] ............ (step 1 of 1) Processing scaler, total=   0.0s\n",
      "7th stock:  000039.XSHE\n",
      "[Pipeline] ............ (step 1 of 1) Processing scaler, total=   0.0s\n",
      "8th stock:  000046.XSHE\n",
      "[Pipeline] ............ (step 1 of 1) Processing scaler, total=   0.0s\n",
      "9th stock:  000060.XSHE\n",
      "[Pipeline] ............ (step 1 of 1) Processing scaler, total=   0.0s\n"
     ]
    }
   ],
   "source": [
    "from fastcore.foundation import L\n",
    "import numpy as np\n",
    "\n",
    "splits = tuple()\n",
    "\n",
    "train_splits = []\n",
    "valid_splits = []\n",
    "test_splits = []\n",
    "\n",
    "X_data = None\n",
    "y_data = None\n",
    "trade_dates = dataset.index.levels[0].tolist()\n",
    "len_dates = len(trade_dates)\n",
    "\n",
    "count = 0\n",
    "for stock in stock_codes:\n",
    "    print(str(count) + 'th stock: ', stock)\n",
    "    stock_df = dataset.loc[(slice(None), stock), :]\n",
    "    # print('stock_df.shape: ', stock_df.shape)\n",
    "    # print('stock_df.shape: ', stock_df.shape)\n",
    "    # print('x_vars: ', x_vars)\n",
    "    # print('y_vars: ', y_vars)\n",
    "    # print(stock_df.head())\n",
    "    stock_df.reset_index(inplace=True)\n",
    "    stock_df = exp_pipe.fit_transform(stock_df.loc[:], scaler__idxs=train_split)\n",
    "    for index in train_split:\n",
    "        train_splits.append(index + count * add_len)\n",
    "    for index in valid_split:\n",
    "        valid_splits.append(index + count * add_len)\n",
    "    for index in test_split:\n",
    "        test_splits.append(index + count * add_len)\n",
    "    count += 1\n",
    "    # print(stock_df.head())\n",
    "    x_vars = stock_df.columns[2:] # 预测的基准变量\n",
    "    y_vars = stock_df.columns[-1] # 要预测的变量\n",
    "    X, y = prepare_forecasting_data(stock_df, fcst_history=fcst_history, fcst_horizon=fcst_horizon, x_vars=x_vars, y_vars=y_vars)\n",
    "    X = X.astype(np.float32)\n",
    "    y = y.astype(np.float32)\n",
    "    X_data = X if X_data is None else np.concatenate((X_data, X), axis=0)\n",
    "    y_data = y if y_data is None else np.concatenate((y_data, y), axis=0)\n",
    "    # print('X_data.shape: ', X_data.shape)\n",
    "    # print('y_data.shape: ', y_data.shape)\n",
    "    if count == 10:\n",
    "        break\n",
    "\n",
    "train_splits = L(train_splits)\n",
    "valid_splits = L(valid_splits)\n",
    "test_splits = L(test_splits)\n",
    "\n",
    "splits += (train_splits,)\n",
    "splits += (valid_splits,)\n",
    "splits += (test_splits,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#15570) [0,1,2,3,4,5,6,7,8,9...],\n",
       " (#2220) [1586,1587,1588,1589,1590,1591,1592,1593,1594,1595...],\n",
       " (#4450) [1837,1838,1839,1840,1841,1842,1843,1844,1845,1846...])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.float32, numpy.float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_data[0][0][0]), type(y_data[0][0][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3 模型设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_config = dict(\n",
    "    n_layers=3,  # number of encoder layers\n",
    "    n_heads=4,  # number of heads\n",
    "    d_model=16,  # dimension of model\n",
    "    d_ff=128,  # dimension of fully connected network\n",
    "    attn_dropout=0.0, # dropout applied to the attention weights\n",
    "    dropout=0.3,  # dropout applied to all linear layers in the encoder except q,k&v projections\n",
    "    patch_len=24,  # length of the patch applied to the time series to create patches\n",
    "    stride=2,  # stride used when creating patches\n",
    "    padding_patch=True,  # padding_patch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = TSForecaster(X_data, y_data, splits=splits, batch_size=16, path=\"models\", pipelines=[preproc_pipe, exp_pipe],\n",
    "                     arch=\"PatchTST\", arch_config=arch_config, metrics=[mse, mae], cbs=ShowGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not enough values to plot a chart\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PatchTST (Input shape: 16 x 51 x 120)\n",
       "============================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "============================================================================\n",
       "                     16 x 51 x 30        \n",
       "RevIN                                     102        True      \n",
       "____________________________________________________________________________\n",
       "                     16 x 51 x 122       \n",
       "ReplicationPad1d                                               \n",
       "____________________________________________________________________________\n",
       "                     16 x 24 x 50        \n",
       "Unfold                                                         \n",
       "____________________________________________________________________________\n",
       "                     16 x 51 x 50 x 16   \n",
       "Linear                                    400        True      \n",
       "Dropout                                                        \n",
       "Linear                                    272        True      \n",
       "Linear                                    272        True      \n",
       "Linear                                    272        True      \n",
       "Dropout                                                        \n",
       "Linear                                    272        True      \n",
       "Dropout                                                        \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     16 x 16 x 50        \n",
       "Transpose                                                      \n",
       "BatchNorm1d                               32         True      \n",
       "____________________________________________________________________________\n",
       "                     16 x 50 x 16        \n",
       "Transpose                                                      \n",
       "____________________________________________________________________________\n",
       "                     16 x 50 x 128       \n",
       "Linear                                    2176       True      \n",
       "GELU                                                           \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     16 x 50 x 16        \n",
       "Linear                                    2064       True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     16 x 16 x 50        \n",
       "Transpose                                                      \n",
       "BatchNorm1d                               32         True      \n",
       "____________________________________________________________________________\n",
       "                     16 x 50 x 16        \n",
       "Transpose                                                      \n",
       "Linear                                    272        True      \n",
       "Linear                                    272        True      \n",
       "Linear                                    272        True      \n",
       "Dropout                                                        \n",
       "Linear                                    272        True      \n",
       "Dropout                                                        \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     16 x 16 x 50        \n",
       "Transpose                                                      \n",
       "BatchNorm1d                               32         True      \n",
       "____________________________________________________________________________\n",
       "                     16 x 50 x 16        \n",
       "Transpose                                                      \n",
       "____________________________________________________________________________\n",
       "                     16 x 50 x 128       \n",
       "Linear                                    2176       True      \n",
       "GELU                                                           \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     16 x 50 x 16        \n",
       "Linear                                    2064       True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     16 x 16 x 50        \n",
       "Transpose                                                      \n",
       "BatchNorm1d                               32         True      \n",
       "____________________________________________________________________________\n",
       "                     16 x 50 x 16        \n",
       "Transpose                                                      \n",
       "Linear                                    272        True      \n",
       "Linear                                    272        True      \n",
       "Linear                                    272        True      \n",
       "Dropout                                                        \n",
       "Linear                                    272        True      \n",
       "Dropout                                                        \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     16 x 16 x 50        \n",
       "Transpose                                                      \n",
       "BatchNorm1d                               32         True      \n",
       "____________________________________________________________________________\n",
       "                     16 x 50 x 16        \n",
       "Transpose                                                      \n",
       "____________________________________________________________________________\n",
       "                     16 x 50 x 128       \n",
       "Linear                                    2176       True      \n",
       "GELU                                                           \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     16 x 50 x 16        \n",
       "Linear                                    2064       True      \n",
       "Dropout                                                        \n",
       "____________________________________________________________________________\n",
       "                     16 x 16 x 50        \n",
       "Transpose                                                      \n",
       "BatchNorm1d                               32         True      \n",
       "____________________________________________________________________________\n",
       "                     16 x 50 x 16        \n",
       "Transpose                                                      \n",
       "____________________________________________________________________________\n",
       "                     16 x 51 x 800       \n",
       "Flatten                                                        \n",
       "____________________________________________________________________________\n",
       "                     16 x 51 x 30        \n",
       "Linear                                    24030      True      \n",
       "____________________________________________________________________________\n",
       "\n",
       "Total params: 40,708\n",
       "Total trainable params: 40,708\n",
       "Total non-trainable params: 0\n",
       "\n",
       "Optimizer used: <function Adam at 0x7fb21e2c1310>\n",
       "Loss function: FlattenedLoss of MSELoss()\n",
       "\n",
       "Callbacks:\n",
       "  - TrainEvalCallback\n",
       "  - CastToTensor\n",
       "  - Recorder\n",
       "  - ProgressCallback\n",
       "  - ShowGraph"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (24480) must match the size of tensor b (480) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[72], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m lr_max \u001B[39m=\u001B[39m \u001B[39m0.001\u001B[39m\n\u001B[1;32m      3\u001B[0m \u001B[39m# lr_max = 0.0025\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m learn\u001B[39m.\u001B[39;49mfit_one_cycle(n_epochs, lr_max\u001B[39m=\u001B[39;49mlr_max)\n\u001B[1;32m      5\u001B[0m learn\u001B[39m.\u001B[39mexport(\u001B[39m'\u001B[39m\u001B[39mpatchTST.pt\u001B[39m\u001B[39m'\u001B[39m)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/quant/lib/python3.8/site-packages/fastai/callback/schedule.py:119\u001B[0m, in \u001B[0;36mfit_one_cycle\u001B[0;34m(self, n_epoch, lr_max, div, div_final, pct_start, wd, moms, cbs, reset_opt, start_epoch)\u001B[0m\n\u001B[1;32m    116\u001B[0m lr_max \u001B[39m=\u001B[39m np\u001B[39m.\u001B[39marray([h[\u001B[39m'\u001B[39m\u001B[39mlr\u001B[39m\u001B[39m'\u001B[39m] \u001B[39mfor\u001B[39;00m h \u001B[39min\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mopt\u001B[39m.\u001B[39mhypers])\n\u001B[1;32m    117\u001B[0m scheds \u001B[39m=\u001B[39m {\u001B[39m'\u001B[39m\u001B[39mlr\u001B[39m\u001B[39m'\u001B[39m: combined_cos(pct_start, lr_max\u001B[39m/\u001B[39mdiv, lr_max, lr_max\u001B[39m/\u001B[39mdiv_final),\n\u001B[1;32m    118\u001B[0m           \u001B[39m'\u001B[39m\u001B[39mmom\u001B[39m\u001B[39m'\u001B[39m: combined_cos(pct_start, \u001B[39m*\u001B[39m(\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mmoms \u001B[39mif\u001B[39;00m moms \u001B[39mis\u001B[39;00m \u001B[39mNone\u001B[39;00m \u001B[39melse\u001B[39;00m moms))}\n\u001B[0;32m--> 119\u001B[0m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mfit(n_epoch, cbs\u001B[39m=\u001B[39;49mParamScheduler(scheds)\u001B[39m+\u001B[39;49mL(cbs), reset_opt\u001B[39m=\u001B[39;49mreset_opt, wd\u001B[39m=\u001B[39;49mwd, start_epoch\u001B[39m=\u001B[39;49mstart_epoch)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/quant/lib/python3.8/site-packages/fastai/learner.py:264\u001B[0m, in \u001B[0;36mLearner.fit\u001B[0;34m(self, n_epoch, lr, wd, cbs, reset_opt, start_epoch)\u001B[0m\n\u001B[1;32m    262\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mopt\u001B[39m.\u001B[39mset_hypers(lr\u001B[39m=\u001B[39m\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mlr \u001B[39mif\u001B[39;00m lr \u001B[39mis\u001B[39;00m \u001B[39mNone\u001B[39;00m \u001B[39melse\u001B[39;00m lr)\n\u001B[1;32m    263\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mn_epoch \u001B[39m=\u001B[39m n_epoch\n\u001B[0;32m--> 264\u001B[0m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_with_events(\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_do_fit, \u001B[39m'\u001B[39;49m\u001B[39mfit\u001B[39;49m\u001B[39m'\u001B[39;49m, CancelFitException, \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_end_cleanup)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/quant/lib/python3.8/site-packages/fastai/learner.py:199\u001B[0m, in \u001B[0;36mLearner._with_events\u001B[0;34m(self, f, event_type, ex, final)\u001B[0m\n\u001B[1;32m    198\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39m_with_events\u001B[39m(\u001B[39mself\u001B[39m, f, event_type, ex, final\u001B[39m=\u001B[39mnoop):\n\u001B[0;32m--> 199\u001B[0m     \u001B[39mtry\u001B[39;00m: \u001B[39mself\u001B[39m(\u001B[39mf\u001B[39m\u001B[39m'\u001B[39m\u001B[39mbefore_\u001B[39m\u001B[39m{\u001B[39;00mevent_type\u001B[39m}\u001B[39;00m\u001B[39m'\u001B[39m);  f()\n\u001B[1;32m    200\u001B[0m     \u001B[39mexcept\u001B[39;00m ex: \u001B[39mself\u001B[39m(\u001B[39mf\u001B[39m\u001B[39m'\u001B[39m\u001B[39mafter_cancel_\u001B[39m\u001B[39m{\u001B[39;00mevent_type\u001B[39m}\u001B[39;00m\u001B[39m'\u001B[39m)\n\u001B[1;32m    201\u001B[0m     \u001B[39mself\u001B[39m(\u001B[39mf\u001B[39m\u001B[39m'\u001B[39m\u001B[39mafter_\u001B[39m\u001B[39m{\u001B[39;00mevent_type\u001B[39m}\u001B[39;00m\u001B[39m'\u001B[39m);  final()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/quant/lib/python3.8/site-packages/fastai/learner.py:253\u001B[0m, in \u001B[0;36mLearner._do_fit\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    251\u001B[0m \u001B[39mfor\u001B[39;00m epoch \u001B[39min\u001B[39;00m \u001B[39mrange\u001B[39m(\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mn_epoch):\n\u001B[1;32m    252\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mepoch\u001B[39m=\u001B[39mepoch\n\u001B[0;32m--> 253\u001B[0m     \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_with_events(\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_do_epoch, \u001B[39m'\u001B[39;49m\u001B[39mepoch\u001B[39;49m\u001B[39m'\u001B[39;49m, CancelEpochException)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/quant/lib/python3.8/site-packages/fastai/learner.py:199\u001B[0m, in \u001B[0;36mLearner._with_events\u001B[0;34m(self, f, event_type, ex, final)\u001B[0m\n\u001B[1;32m    198\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39m_with_events\u001B[39m(\u001B[39mself\u001B[39m, f, event_type, ex, final\u001B[39m=\u001B[39mnoop):\n\u001B[0;32m--> 199\u001B[0m     \u001B[39mtry\u001B[39;00m: \u001B[39mself\u001B[39m(\u001B[39mf\u001B[39m\u001B[39m'\u001B[39m\u001B[39mbefore_\u001B[39m\u001B[39m{\u001B[39;00mevent_type\u001B[39m}\u001B[39;00m\u001B[39m'\u001B[39m);  f()\n\u001B[1;32m    200\u001B[0m     \u001B[39mexcept\u001B[39;00m ex: \u001B[39mself\u001B[39m(\u001B[39mf\u001B[39m\u001B[39m'\u001B[39m\u001B[39mafter_cancel_\u001B[39m\u001B[39m{\u001B[39;00mevent_type\u001B[39m}\u001B[39;00m\u001B[39m'\u001B[39m)\n\u001B[1;32m    201\u001B[0m     \u001B[39mself\u001B[39m(\u001B[39mf\u001B[39m\u001B[39m'\u001B[39m\u001B[39mafter_\u001B[39m\u001B[39m{\u001B[39;00mevent_type\u001B[39m}\u001B[39;00m\u001B[39m'\u001B[39m);  final()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/quant/lib/python3.8/site-packages/fastai/learner.py:247\u001B[0m, in \u001B[0;36mLearner._do_epoch\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    246\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39m_do_epoch\u001B[39m(\u001B[39mself\u001B[39m):\n\u001B[0;32m--> 247\u001B[0m     \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_do_epoch_train()\n\u001B[1;32m    248\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_do_epoch_validate()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/quant/lib/python3.8/site-packages/fastai/learner.py:239\u001B[0m, in \u001B[0;36mLearner._do_epoch_train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    237\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39m_do_epoch_train\u001B[39m(\u001B[39mself\u001B[39m):\n\u001B[1;32m    238\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdl \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdls\u001B[39m.\u001B[39mtrain\n\u001B[0;32m--> 239\u001B[0m     \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_with_events(\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mall_batches, \u001B[39m'\u001B[39;49m\u001B[39mtrain\u001B[39;49m\u001B[39m'\u001B[39;49m, CancelTrainException)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/quant/lib/python3.8/site-packages/fastai/learner.py:199\u001B[0m, in \u001B[0;36mLearner._with_events\u001B[0;34m(self, f, event_type, ex, final)\u001B[0m\n\u001B[1;32m    198\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39m_with_events\u001B[39m(\u001B[39mself\u001B[39m, f, event_type, ex, final\u001B[39m=\u001B[39mnoop):\n\u001B[0;32m--> 199\u001B[0m     \u001B[39mtry\u001B[39;00m: \u001B[39mself\u001B[39m(\u001B[39mf\u001B[39m\u001B[39m'\u001B[39m\u001B[39mbefore_\u001B[39m\u001B[39m{\u001B[39;00mevent_type\u001B[39m}\u001B[39;00m\u001B[39m'\u001B[39m);  f()\n\u001B[1;32m    200\u001B[0m     \u001B[39mexcept\u001B[39;00m ex: \u001B[39mself\u001B[39m(\u001B[39mf\u001B[39m\u001B[39m'\u001B[39m\u001B[39mafter_cancel_\u001B[39m\u001B[39m{\u001B[39;00mevent_type\u001B[39m}\u001B[39;00m\u001B[39m'\u001B[39m)\n\u001B[1;32m    201\u001B[0m     \u001B[39mself\u001B[39m(\u001B[39mf\u001B[39m\u001B[39m'\u001B[39m\u001B[39mafter_\u001B[39m\u001B[39m{\u001B[39;00mevent_type\u001B[39m}\u001B[39;00m\u001B[39m'\u001B[39m);  final()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/quant/lib/python3.8/site-packages/fastai/learner.py:205\u001B[0m, in \u001B[0;36mLearner.all_batches\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    203\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mall_batches\u001B[39m(\u001B[39mself\u001B[39m):\n\u001B[1;32m    204\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mn_iter \u001B[39m=\u001B[39m \u001B[39mlen\u001B[39m(\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdl)\n\u001B[0;32m--> 205\u001B[0m     \u001B[39mfor\u001B[39;00m o \u001B[39min\u001B[39;00m \u001B[39menumerate\u001B[39m(\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdl): \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mone_batch(\u001B[39m*\u001B[39;49mo)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/quant/lib/python3.8/site-packages/tsai/learner.py:40\u001B[0m, in \u001B[0;36mone_batch\u001B[0;34m(self, i, b)\u001B[0m\n\u001B[1;32m     38\u001B[0m b_on_device \u001B[39m=\u001B[39m to_device(b, device\u001B[39m=\u001B[39m\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdls\u001B[39m.\u001B[39mdevice) \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdls\u001B[39m.\u001B[39mdevice \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m \u001B[39melse\u001B[39;00m b\n\u001B[1;32m     39\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_split(b_on_device)\n\u001B[0;32m---> 40\u001B[0m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_with_events(\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_do_one_batch, \u001B[39m'\u001B[39;49m\u001B[39mbatch\u001B[39;49m\u001B[39m'\u001B[39;49m, CancelBatchException)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/quant/lib/python3.8/site-packages/fastai/learner.py:199\u001B[0m, in \u001B[0;36mLearner._with_events\u001B[0;34m(self, f, event_type, ex, final)\u001B[0m\n\u001B[1;32m    198\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39m_with_events\u001B[39m(\u001B[39mself\u001B[39m, f, event_type, ex, final\u001B[39m=\u001B[39mnoop):\n\u001B[0;32m--> 199\u001B[0m     \u001B[39mtry\u001B[39;00m: \u001B[39mself\u001B[39m(\u001B[39mf\u001B[39m\u001B[39m'\u001B[39m\u001B[39mbefore_\u001B[39m\u001B[39m{\u001B[39;00mevent_type\u001B[39m}\u001B[39;00m\u001B[39m'\u001B[39m);  f()\n\u001B[1;32m    200\u001B[0m     \u001B[39mexcept\u001B[39;00m ex: \u001B[39mself\u001B[39m(\u001B[39mf\u001B[39m\u001B[39m'\u001B[39m\u001B[39mafter_cancel_\u001B[39m\u001B[39m{\u001B[39;00mevent_type\u001B[39m}\u001B[39;00m\u001B[39m'\u001B[39m)\n\u001B[1;32m    201\u001B[0m     \u001B[39mself\u001B[39m(\u001B[39mf\u001B[39m\u001B[39m'\u001B[39m\u001B[39mafter_\u001B[39m\u001B[39m{\u001B[39;00mevent_type\u001B[39m}\u001B[39;00m\u001B[39m'\u001B[39m);  final()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/quant/lib/python3.8/site-packages/fastai/learner.py:219\u001B[0m, in \u001B[0;36mLearner._do_one_batch\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    217\u001B[0m \u001B[39mself\u001B[39m(\u001B[39m'\u001B[39m\u001B[39mafter_pred\u001B[39m\u001B[39m'\u001B[39m)\n\u001B[1;32m    218\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mlen\u001B[39m(\u001B[39mself\u001B[39m\u001B[39m.\u001B[39myb):\n\u001B[0;32m--> 219\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mloss_grad \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mloss_func(\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mpred, \u001B[39m*\u001B[39;49m\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49myb)\n\u001B[1;32m    220\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mloss \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mloss_grad\u001B[39m.\u001B[39mclone()\n\u001B[1;32m    221\u001B[0m \u001B[39mself\u001B[39m(\u001B[39m'\u001B[39m\u001B[39mafter_loss\u001B[39m\u001B[39m'\u001B[39m)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/quant/lib/python3.8/site-packages/fastai/losses.py:54\u001B[0m, in \u001B[0;36mBaseLoss.__call__\u001B[0;34m(self, inp, targ, **kwargs)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[39mif\u001B[39;00m targ\u001B[39m.\u001B[39mdtype \u001B[39min\u001B[39;00m [torch\u001B[39m.\u001B[39mint8, torch\u001B[39m.\u001B[39mint16, torch\u001B[39m.\u001B[39mint32]: targ \u001B[39m=\u001B[39m targ\u001B[39m.\u001B[39mlong()\n\u001B[1;32m     53\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mflatten: inp \u001B[39m=\u001B[39m inp\u001B[39m.\u001B[39mview(\u001B[39m-\u001B[39m\u001B[39m1\u001B[39m,inp\u001B[39m.\u001B[39mshape[\u001B[39m-\u001B[39m\u001B[39m1\u001B[39m]) \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mis_2d \u001B[39melse\u001B[39;00m inp\u001B[39m.\u001B[39mview(\u001B[39m-\u001B[39m\u001B[39m1\u001B[39m)\n\u001B[0;32m---> 54\u001B[0m \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mfunc\u001B[39m.\u001B[39;49m\u001B[39m__call__\u001B[39;49m(inp, targ\u001B[39m.\u001B[39;49mview(\u001B[39m-\u001B[39;49m\u001B[39m1\u001B[39;49m) \u001B[39mif\u001B[39;49;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mflatten \u001B[39melse\u001B[39;49;00m targ, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/quant/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[39m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m (\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_pre_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[39mor\u001B[39;00m _global_backward_pre_hooks \u001B[39mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[39mor\u001B[39;00m _global_forward_hooks \u001B[39mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[39mreturn\u001B[39;00m forward_call(\u001B[39m*\u001B[39;49margs, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n\u001B[1;32m   1502\u001B[0m \u001B[39m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[39m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/quant/lib/python3.8/site-packages/torch/nn/modules/loss.py:536\u001B[0m, in \u001B[0;36mMSELoss.forward\u001B[0;34m(self, input, target)\u001B[0m\n\u001B[1;32m    535\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mforward\u001B[39m(\u001B[39mself\u001B[39m, \u001B[39minput\u001B[39m: Tensor, target: Tensor) \u001B[39m-\u001B[39m\u001B[39m>\u001B[39m Tensor:\n\u001B[0;32m--> 536\u001B[0m     \u001B[39mreturn\u001B[39;00m F\u001B[39m.\u001B[39;49mmse_loss(\u001B[39minput\u001B[39;49m, target, reduction\u001B[39m=\u001B[39;49m\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mreduction)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/quant/lib/python3.8/site-packages/torch/nn/functional.py:3281\u001B[0m, in \u001B[0;36mmse_loss\u001B[0;34m(input, target, size_average, reduce, reduction)\u001B[0m\n\u001B[1;32m   3274\u001B[0m \u001B[39mr\u001B[39m\u001B[39m\"\"\"mse_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor\u001B[39;00m\n\u001B[1;32m   3275\u001B[0m \n\u001B[1;32m   3276\u001B[0m \u001B[39mMeasures the element-wise mean squared error.\u001B[39;00m\n\u001B[1;32m   3277\u001B[0m \n\u001B[1;32m   3278\u001B[0m \u001B[39mSee :class:`~torch.nn.MSELoss` for details.\u001B[39;00m\n\u001B[1;32m   3279\u001B[0m \u001B[39m\"\"\"\u001B[39;00m\n\u001B[1;32m   3280\u001B[0m \u001B[39mif\u001B[39;00m has_torch_function_variadic(\u001B[39minput\u001B[39m, target):\n\u001B[0;32m-> 3281\u001B[0m     \u001B[39mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m   3282\u001B[0m         mse_loss, (\u001B[39minput\u001B[39;49m, target), \u001B[39minput\u001B[39;49m, target, size_average\u001B[39m=\u001B[39;49msize_average, reduce\u001B[39m=\u001B[39;49mreduce, reduction\u001B[39m=\u001B[39;49mreduction\n\u001B[1;32m   3283\u001B[0m     )\n\u001B[1;32m   3284\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m (target\u001B[39m.\u001B[39msize() \u001B[39m==\u001B[39m \u001B[39minput\u001B[39m\u001B[39m.\u001B[39msize()):\n\u001B[1;32m   3285\u001B[0m     warnings\u001B[39m.\u001B[39mwarn(\n\u001B[1;32m   3286\u001B[0m         \u001B[39m\"\u001B[39m\u001B[39mUsing a target size (\u001B[39m\u001B[39m{}\u001B[39;00m\u001B[39m) that is different to the input size (\u001B[39m\u001B[39m{}\u001B[39;00m\u001B[39m). \u001B[39m\u001B[39m\"\u001B[39m\n\u001B[1;32m   3287\u001B[0m         \u001B[39m\"\u001B[39m\u001B[39mThis will likely lead to incorrect results due to broadcasting. \u001B[39m\u001B[39m\"\u001B[39m\n\u001B[1;32m   3288\u001B[0m         \u001B[39m\"\u001B[39m\u001B[39mPlease ensure they have the same size.\u001B[39m\u001B[39m\"\u001B[39m\u001B[39m.\u001B[39mformat(target\u001B[39m.\u001B[39msize(), \u001B[39minput\u001B[39m\u001B[39m.\u001B[39msize()),\n\u001B[1;32m   3289\u001B[0m         stacklevel\u001B[39m=\u001B[39m\u001B[39m2\u001B[39m,\n\u001B[1;32m   3290\u001B[0m     )\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/quant/lib/python3.8/site-packages/torch/overrides.py:1551\u001B[0m, in \u001B[0;36mhandle_torch_function\u001B[0;34m(public_api, relevant_args, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1545\u001B[0m     warnings\u001B[39m.\u001B[39mwarn(\u001B[39m\"\u001B[39m\u001B[39mDefining your `__torch_function__ as a plain method is deprecated and \u001B[39m\u001B[39m\"\u001B[39m\n\u001B[1;32m   1546\u001B[0m                   \u001B[39m\"\u001B[39m\u001B[39mwill be an error in future, please define it as a classmethod.\u001B[39m\u001B[39m\"\u001B[39m,\n\u001B[1;32m   1547\u001B[0m                   \u001B[39mDeprecationWarning\u001B[39;00m)\n\u001B[1;32m   1549\u001B[0m \u001B[39m# Use `public_api` instead of `implementation` so __torch_function__\u001B[39;00m\n\u001B[1;32m   1550\u001B[0m \u001B[39m# implementations can do equality/identity comparisons.\u001B[39;00m\n\u001B[0;32m-> 1551\u001B[0m result \u001B[39m=\u001B[39m torch_func_method(public_api, types, args, kwargs)\n\u001B[1;32m   1553\u001B[0m \u001B[39mif\u001B[39;00m result \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNotImplemented\u001B[39m:\n\u001B[1;32m   1554\u001B[0m     \u001B[39mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/quant/lib/python3.8/site-packages/fastai/torch_core.py:382\u001B[0m, in \u001B[0;36mTensorBase.__torch_function__\u001B[0;34m(cls, func, types, args, kwargs)\u001B[0m\n\u001B[1;32m    380\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mcls\u001B[39m\u001B[39m.\u001B[39mdebug \u001B[39mand\u001B[39;00m func\u001B[39m.\u001B[39m\u001B[39m__name__\u001B[39m \u001B[39mnot\u001B[39;00m \u001B[39min\u001B[39;00m (\u001B[39m'\u001B[39m\u001B[39m__str__\u001B[39m\u001B[39m'\u001B[39m,\u001B[39m'\u001B[39m\u001B[39m__repr__\u001B[39m\u001B[39m'\u001B[39m): \u001B[39mprint\u001B[39m(func, types, args, kwargs)\n\u001B[1;32m    381\u001B[0m \u001B[39mif\u001B[39;00m _torch_handled(args, \u001B[39mcls\u001B[39m\u001B[39m.\u001B[39m_opt, func): types \u001B[39m=\u001B[39m (torch\u001B[39m.\u001B[39mTensor,)\n\u001B[0;32m--> 382\u001B[0m res \u001B[39m=\u001B[39m \u001B[39msuper\u001B[39;49m()\u001B[39m.\u001B[39;49m__torch_function__(func, types, args, ifnone(kwargs, {}))\n\u001B[1;32m    383\u001B[0m dict_objs \u001B[39m=\u001B[39m _find_args(args) \u001B[39mif\u001B[39;00m args \u001B[39melse\u001B[39;00m _find_args(\u001B[39mlist\u001B[39m(kwargs\u001B[39m.\u001B[39mvalues()))\n\u001B[1;32m    384\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39missubclass\u001B[39m(\u001B[39mtype\u001B[39m(res),TensorBase) \u001B[39mand\u001B[39;00m dict_objs: res\u001B[39m.\u001B[39mset_meta(dict_objs[\u001B[39m0\u001B[39m],as_copy\u001B[39m=\u001B[39m\u001B[39mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/quant/lib/python3.8/site-packages/torch/_tensor.py:1295\u001B[0m, in \u001B[0;36mTensor.__torch_function__\u001B[0;34m(cls, func, types, args, kwargs)\u001B[0m\n\u001B[1;32m   1292\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mNotImplemented\u001B[39m\n\u001B[1;32m   1294\u001B[0m \u001B[39mwith\u001B[39;00m _C\u001B[39m.\u001B[39mDisableTorchFunctionSubclass():\n\u001B[0;32m-> 1295\u001B[0m     ret \u001B[39m=\u001B[39m func(\u001B[39m*\u001B[39;49margs, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n\u001B[1;32m   1296\u001B[0m     \u001B[39mif\u001B[39;00m func \u001B[39min\u001B[39;00m get_default_nowrap_functions():\n\u001B[1;32m   1297\u001B[0m         \u001B[39mreturn\u001B[39;00m ret\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/quant/lib/python3.8/site-packages/torch/nn/functional.py:3294\u001B[0m, in \u001B[0;36mmse_loss\u001B[0;34m(input, target, size_average, reduce, reduction)\u001B[0m\n\u001B[1;32m   3291\u001B[0m \u001B[39mif\u001B[39;00m size_average \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m \u001B[39mor\u001B[39;00m reduce \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n\u001B[1;32m   3292\u001B[0m     reduction \u001B[39m=\u001B[39m _Reduction\u001B[39m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[0;32m-> 3294\u001B[0m expanded_input, expanded_target \u001B[39m=\u001B[39m torch\u001B[39m.\u001B[39;49mbroadcast_tensors(\u001B[39minput\u001B[39;49m, target)\n\u001B[1;32m   3295\u001B[0m \u001B[39mreturn\u001B[39;00m torch\u001B[39m.\u001B[39m_C\u001B[39m.\u001B[39m_nn\u001B[39m.\u001B[39mmse_loss(expanded_input, expanded_target, _Reduction\u001B[39m.\u001B[39mget_enum(reduction))\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/quant/lib/python3.8/site-packages/torch/functional.py:74\u001B[0m, in \u001B[0;36mbroadcast_tensors\u001B[0;34m(*tensors)\u001B[0m\n\u001B[1;32m     72\u001B[0m \u001B[39mif\u001B[39;00m has_torch_function(tensors):\n\u001B[1;32m     73\u001B[0m     \u001B[39mreturn\u001B[39;00m handle_torch_function(broadcast_tensors, tensors, \u001B[39m*\u001B[39mtensors)\n\u001B[0;32m---> 74\u001B[0m \u001B[39mreturn\u001B[39;00m _VF\u001B[39m.\u001B[39;49mbroadcast_tensors(tensors)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: The size of tensor a (24480) must match the size of tensor b (480) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "lr_max = 0.001\n",
    "# lr_max = 0.0025\n",
    "learn.fit_one_cycle(n_epochs, lr_max=lr_max)\n",
    "learn.export('patchTST.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsai.inference import load_learner\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "learn = load_learner('models/patchTST.pt')\n",
    "scaled_preds, *_ = learn.get_X_preds(X_data[splits[1]])\n",
    "scaled_preds = to_np(scaled_preds)\n",
    "print(f\"scaled_preds.shape: {scaled_preds.shape}\")\n",
    "\n",
    "scaled_y_true = y_data[splits[1]]\n",
    "results_df = pd.DataFrame(columns=[\"mse\", \"mae\"])\n",
    "results_df.loc[\"valid\", \"mse\"] = mean_squared_error(scaled_y_true.flatten(), scaled_preds.flatten())\n",
    "results_df.loc[\"valid\", \"mae\"] = mean_absolute_error(scaled_y_true.flatten(), scaled_preds.flatten())\n",
    "results_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型测试\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsai.inference import load_learner\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "learn = load_learner('models/patchTST.pt')\n",
    "y_test_preds, *_ = learn.get_X_preds(X_data[splits[2]])\n",
    "y_test_preds = to_np(y_test_preds)\n",
    "print(f\"y_test_preds.shape: {y_test_preds.shape}\")\n",
    "\n",
    "y_test = y_data[splits[2]]\n",
    "results_df = pd.DataFrame(columns=[\"mse\", \"mae\"])\n",
    "results_df.loc[\"test\", \"mse\"] = mean_squared_error(y_test.flatten(), y_test_preds.flatten())\n",
    "results_df.loc[\"test\", \"mae\"] = mean_absolute_error(y_test.flatten(), y_test_preds.flatten())\n",
    "results_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_data[splits[2]]\n",
    "y_test = y_data[splits[2]]\n",
    "plot_forecast(X_test, y_test, y_test_preds, sel_vars=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型实际应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
